# fixed_understanding_trainer_v3.py
import os
import random
import math
import json
import torch
from typing import List, Dict, Optional
from datetime import datetime

import datasets as hf_datasets
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, TaskType, get_peft_model

from backend.elasticsearch_client_v8 import ElasticsearchClient

class ImprovedUnderstandingDataGenerator:
    """GeliÅŸtirilmiÅŸ anlama odaklÄ± veri Ã¼reticisi"""
    
    def __init__(self, es_client=None):
        self.es_client = es_client
        
    def generate_comprehensive_training_data(self):
        """KapsamlÄ± eÄŸitim verisi Ã¼retir"""
        
        all_pairs = []
        
        # 1. Temel kavram tanÄ±mlarÄ±
        all_pairs.extend(self._generate_basic_concepts())
        
        # 2. Veri yapÄ±sÄ± Ã¶ÄŸretimi
        all_pairs.extend(self._generate_data_structure_examples())
        
        # 3. Sorgu anlama patterns
        all_pairs.extend(self._generate_query_patterns())
        
        # 4. Zaman anlama
        all_pairs.extend(self._generate_time_understanding())
        
        # 5. BaÄŸlam anlama
        all_pairs.extend(self._generate_context_examples())
        
        # 6. GerÃ§ek veri Ã¶rnekleri (varsa)
        if self.es_client:
            try:
                all_pairs.extend(self._generate_real_data_examples())
            except Exception as e:
                print(f"âš ï¸ GerÃ§ek veri Ã¶rnekleri eklenemedi: {e}")
        
        print(f"ğŸ“Š Toplam {len(all_pairs)} eÄŸitim Ã¶rneÄŸi Ã¼retildi")
        return all_pairs
    
    def _generate_basic_concepts(self):
        """Temel kavramlarÄ± Ã¶ÄŸret"""
        return [
            {
                "input": "CPU kullanÄ±mÄ± %75 ne anlama gelir?",
                "output": "CPU kullanÄ±mÄ± %75, iÅŸlemcinin orta-yÃ¼ksek seviyede Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterir. Normal kullanÄ±m iÃ§in biraz yÃ¼ksek sayÄ±lÄ±r."
            },
            {
                "input": "RAM %90 dolu olmasÄ± ne demek?",
                "output": "RAM %90 dolu olmasÄ±, belleÄŸin kritik seviyede olduÄŸunu gÃ¶sterir. Sistem yavaÅŸlayabilir ve swap memory kullanmaya baÅŸlayabilir."
            },
            {
                "input": "VPN detected ne anlama gelir?",
                "output": "VPN detected, kullanÄ±cÄ±nÄ±n VPN (Virtual Private Network) kullandÄ±ÄŸÄ±nÄ±, yani IP adresinin gizlendiÄŸini gÃ¶sterir."
            },
            {
                "input": "Ping 150ms yÃ¼ksek mi?",
                "output": "Evet, 150ms ping yÃ¼ksek sayÄ±lÄ±r. Ä°yi bir baÄŸlantÄ± iÃ§in 50ms altÄ±, kabul edilebilir iÃ§in 100ms altÄ± olmalÄ±dÄ±r."
            },
            {
                "input": "Download speed 5 Mbps nasÄ±l?",
                "output": "5 Mbps indirme hÄ±zÄ± dÃ¼ÅŸÃ¼k sayÄ±lÄ±r. Temel internet kullanÄ±mÄ± iÃ§in minimum, video izleme iÃ§in yetersiz olabilir."
            },
            {
                "input": "Disk kullanÄ±mÄ± %95 tehlikeli mi?",
                "output": "Evet, disk kullanÄ±mÄ± %95 tehlikelidir. Sistem donabilir, dosya kaydetme sorunlarÄ± yaÅŸanabilir. Acil disk temizliÄŸi gerekir."
            },
            {
                "input": "sistem yavaÅŸ ne demek?",
                "output": "Sistem yavaÅŸ genellikle CPU >%80, RAM >%90 veya disk >%95 olduÄŸunda ortaya Ã§Ä±kar. Birden fazla metrik yÃ¼ksekse kesin yavaÅŸtÄ±r."
            },
            {
                "input": "VPN kapalÄ± mÄ±?",
                "output": "VPN durumunu kontrol etmek iÃ§in web_data.vpn_detection.status alanÄ±na bakmak gerekir. 'no_vpn' ise kapalÄ±, 'vpn_detected' ise aÃ§Ä±k."
            }
        ]
    
    def _generate_data_structure_examples(self):
        """Veri yapÄ±sÄ± Ã¶rnekleri"""
        sample_data = {
            "collection_timestamp": "2024-01-15T14:30:00",
            "system_data": {
                "cpu": {"cpu_percent": 68.5, "core_count": 8},
                "memory": {
                    "virtual_memory": {
                        "percent": 72.1,
                        "used": 7516192768,
                        "total": 10737418240
                    }
                },
                "disk": {
                    "disk_usage": {
                        "main": {"percent": 45.8, "used": 107374182400, "total": 234567890123}
                    }
                }
            },
            "web_data": {
                "ip_address": "185.123.45.67",
                "vpn_detection": {"status": "no_vpn", "message": "Normal ISP connection"},
                "ip_info": {"city": "Istanbul", "country": "Turkey"},
                "speed_test": {"download_speed": 28.5, "upload_speed": 6.2, "ping": 22}
            }
        }
        
        return [
            {
                "input": f"Bu veride CPU bilgisi nerede?\n{json.dumps(sample_data['system_data']['cpu'], ensure_ascii=False)}",
                "output": "CPU bilgisi system_data.cpu.cpu_percent alanÄ±nda bulunur. Bu Ã¶rnekte %68.5 CPU kullanÄ±mÄ± var, normal seviye."
            },
            {
                "input": f"Bu veride VPN kullanÄ±lÄ±yor mu?\n{json.dumps(sample_data['web_data']['vpn_detection'], ensure_ascii=False)}",
                "output": "HayÄ±r, VPN kullanÄ±lmÄ±yor. vpn_detection.status 'no_vpn' olarak gÃ¶zÃ¼kÃ¼yor ve 'Normal ISP connection' mesajÄ± var."
            },
            {
                "input": f"Bu sistemin bellek durumu nasÄ±l?\n{json.dumps(sample_data['system_data']['memory'], ensure_ascii=False)}",
                "output": "Bellek %72.1 dolu (yaklaÅŸÄ±k 7 GB / 10 GB kullanÄ±lÄ±yor). Bu normal kullanÄ±m seviyesi, sorun yok."
            },
            {
                "input": f"Bu internet baÄŸlantÄ±sÄ± nasÄ±l?\n{json.dumps(sample_data['web_data']['speed_test'], ensure_ascii=False)}",
                "output": "Ä°nternet baÄŸlantÄ±sÄ± iyi durumda. 28.5 Mbps indirme hÄ±zÄ± yeterli, 22ms ping dÃ¼ÅŸÃ¼k, 6.2 Mbps yÃ¼kleme hÄ±zÄ± normal."
            },
            {
                "input": "system_data.cpu.cpu_percent nerede bulunur?",
                "output": "system_data.cpu.cpu_percent, JSON verisinin sistem bilgileri bÃ¶lÃ¼mÃ¼nde CPU kullanÄ±m yÃ¼zdesini gÃ¶sterir."
            }
        ]
    
    def _generate_query_patterns(self):
        """Sorgu kalÄ±plarÄ±nÄ± Ã¶ÄŸret"""
        return [
            {
                "input": "son 2 saatte VPN kullandÄ±m mÄ±?",
                "output": "Bu sorgu son 2 saatlik zaman aralÄ±ÄŸÄ±nda VPN durumunu sorgular. web_data.vpn_detection.status alanÄ±nda 'vpn_detected' aranmalÄ±."
            },
            {
                "input": "bugÃ¼n sistem yavaÅŸ mÄ±ydÄ±?",
                "output": "BugÃ¼nkÃ¼ verilerde CPU >%80, RAM >%90 veya disk >%95 deÄŸerlerini kontrol etmek gerekir. Birden fazla metrik yÃ¼ksekse sistem yavaÅŸtÄ±r."
            },
            {
                "input": "14:30'da internet hÄ±zÄ±m ne kadardÄ±?",
                "output": "14:30 civarÄ±ndaki (Â±30 dakika) verilerde web_data.speed_test.download_speed deÄŸerine bakmak gerekir."
            },
            {
                "input": "hangi Ã¼lkede gÃ¶rÃ¼nÃ¼yordum?",
                "output": "En son verideki web_data.ip_info.country alanÄ±na bakmak gerekir. Bu konum bilgisini verir."
            },
            {
                "input": "bu hafta hangi bilgisayarlarÄ±n verisi var?",
                "output": "Bu haftalÄ±k zaman aralÄ±ÄŸÄ±nda device_id ve user_id alanlarÄ±ndaki benzersiz deÄŸerleri listelemek gerekir."
            },
            {
                "input": "bilgisayarÄ±m Ã§ok yavaÅŸ",
                "output": "Sistem performansÄ±nÄ± kontrol etmeliyim: CPU kullanÄ±mÄ±, RAM kullanÄ±mÄ±, disk kullanÄ±mÄ± ve Ã§alÄ±ÅŸan iÅŸlem sayÄ±sÄ±nÄ± incelemeliyim."
            },
            {
                "input": "internet problemim var",
                "output": "AÄŸ durumunu kontrol etmeliyim: internet hÄ±zÄ± testi sonuÃ§larÄ±, ping deÄŸeri, IP adresi deÄŸiÅŸiklikleri ve VPN durumunu incelemeliyim."
            }
        ]
    
    def _generate_time_understanding(self):
        """Zaman ifadelerini Ã¶ÄŸret"""
        return [
            {
                "input": "son 1 saat ne demek?",
                "output": "Son 1 saat, ÅŸu andan 1 saat geriye giden zaman aralÄ±ÄŸÄ±. Elasticsearch range query ile collection_timestamp alanÄ±nda arama yapÄ±lÄ±r."
            },
            {
                "input": "bugÃ¼n ne zaman?",
                "output": "BugÃ¼n, gÃ¼nÃ¼n baÅŸÄ±ndan (00:00) ÅŸu ana kadar olan zaman aralÄ±ÄŸÄ±."
            },
            {
                "input": "dÃ¼n akÅŸam ne zaman?",
                "output": "DÃ¼n akÅŸam, Ã¶nceki gÃ¼nÃ¼n 18:00-23:59 saatleri arasÄ± zaman dilimi."
            },
            {
                "input": "bu sabah ne zaman?",
                "output": "Bu sabah, bugÃ¼nÃ¼n 06:00-12:00 saatleri arasÄ± zaman dilimi."
            },
            {
                "input": "geÃ§en hafta ne zaman?",
                "output": "GeÃ§en hafta, 7 gÃ¼n Ã¶ncesinden bugÃ¼ne kadar olan zaman aralÄ±ÄŸÄ±."
            }
        ]
    
    def _generate_context_examples(self):
        """BaÄŸlam anlama Ã¶rnekleri"""
        return [
            {
                "input": "performans nasÄ±l?",
                "output": "Sistem performansÄ± iÃ§in CPU, RAM ve disk kullanÄ±mÄ±na bakmam gerekir. Bu Ã¼Ã§ metriÄŸi de kontrol ederim."
            },
            {
                "input": "gÃ¼venlik durumu nedir?",
                "output": "GÃ¼venlik iÃ§in VPN durumu, IP adresi lokasyonu ve normal dÄ±ÅŸÄ± aÄŸ aktivitesi olup olmadÄ±ÄŸÄ±nÄ± kontrol ederim."
            },
            {
                "input": "baÄŸlantÄ± problemi var mÄ±?",
                "output": "BaÄŸlantÄ± problemini kontrol etmek iÃ§in internet hÄ±zÄ±, ping deÄŸeri, IP deÄŸiÅŸiklikleri ve VPN durumunu incelerim."
            }
        ]
    
    def _generate_real_data_examples(self):
        """GerÃ§ek veriden Ã¶rnekler Ã¼ret"""
        try:
            # Son 20 kaydÄ± al
            query = {"query": {"match_all": {}}, "size": 20}
            results = self.es_client.search("combined-monitoring", query["query"], size=20)
            
            real_examples = []
            
            for hit in results[:5]:  # Ä°lk 5 kayÄ±t yeterli
                data = hit['_source']
                timestamp = data.get('collection_timestamp', '')[:16]
                
                # CPU Ã¶rneÄŸi
                if data.get('system_data', {}).get('cpu', {}).get('cpu_percent'):
                    cpu_val = data['system_data']['cpu']['cpu_percent']
                    
                    real_examples.append({
                        "input": f"{timestamp} tarihinde CPU kullanÄ±mÄ±m nasÄ±ldÄ±?",
                        "output": f"{timestamp} tarihinde CPU kullanÄ±mÄ±nÄ±z %{cpu_val}{'du. YÃ¼ksek seviye.' if cpu_val > 80 else 'di. Normal seviye.' if cpu_val > 50 else 'di. DÃ¼ÅŸÃ¼k seviye.'}"
                    })
                
                # VPN Ã¶rneÄŸi
                vpn_info = data.get('web_data', {}).get('vpn_detection', {})
                if vpn_info:
                    status = vpn_info.get('status', 'unknown')
                    real_examples.append({
                        "input": f"{timestamp} tarihinde VPN kullanÄ±yor muydum?",
                        "output": f"{timestamp} tarihinde VPN durumunuz: {status}. {'VPN aktifti.' if status == 'vpn_detected' else 'VPN kullanmÄ±yordunuz.' if status == 'no_vpn' else 'Durum belirsiz.'}"
                    })
            
            return real_examples
            
        except Exception as e:
            print(f"âŒ GerÃ§ek veri Ã¶rnekleri oluÅŸturulamadÄ±: {e}")
            return []

def create_fixed_training_args(output_dir: str = "./qwen_understanding_model"):
    """DÃ¼zeltilmiÅŸ eÄŸitim parametreleri"""
    return TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=1,  # KÃ¼Ã§Ã¼k batch size
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=8,  # BÃ¼yÃ¼k effective batch iÃ§in
        learning_rate=2e-5,
        warmup_steps=50,
        eval_strategy="steps",
        eval_steps=50,
        save_steps=100,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        logging_steps=10,
        save_total_limit=2,
        weight_decay=0.01,
        warmup_ratio=0.1,
        dataloader_pin_memory=False,
        fp16=torch.cuda.is_available(),
        dataloader_num_workers=0,
        remove_unused_columns=False,
        report_to=[],
        optim="adamw_torch",
        save_safetensors=True,
        seed=42,
        prediction_loss_only=True,
        group_by_length=True,  # Benzer uzunluklarÄ± grupla
    )

def create_optimized_lora_config():
    """Optimize edilmiÅŸ LoRA konfigÃ¼rasyonu"""
    return LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        lora_alpha=16,
        lora_dropout=0.05,
        target_modules=[
            "q_proj",
            "k_proj", 
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
        bias="none",
        inference_mode=False,
    )

def tokenize_function(examples, tokenizer, max_length=512):
    """DÃœZELTILMIÅ Tokenization fonksiyonu"""
    
    # Girdi metinlerini al
    texts = examples["text"]
    
    # Her metin iÃ§in tokenize et - padding=False, truncation=True
    model_inputs = tokenizer(
        texts,
        truncation=True,
        padding=False,  # Burada padding yapmÄ±yoruz, DataCollator yapacak
        max_length=max_length,
        return_tensors=None  # Tensor olarak deÄŸil, liste olarak dÃ¶ndÃ¼r
    )
    
    # Labels'Ä± input_ids'in kopyasÄ± yap
    model_inputs["labels"] = [ids.copy() for ids in model_inputs["input_ids"]]
    
    return model_inputs

def format_training_example(pair: Dict[str, str]) -> str:
    """EÄŸitim Ã¶rneÄŸini format'la"""
    input_text = pair.get("input", "").strip()
    output_text = pair.get("output", "").strip()
    
    # Qwen instruct format
    return f"<|im_start|>user\n{input_text}<|im_end|>\n<|im_start|>assistant\n{output_text}<|im_end|>"

def prepare_dataset(training_pairs: List[Dict], tokenizer, max_length: int = 512):
    """DÃœZELTILMIÅ Dataset hazÄ±rlama"""
    
    # Train/validation bÃ¶l
    random.shuffle(training_pairs)
    split_idx = max(1, int(len(training_pairs) * 0.85))
    train_pairs = training_pairs[:split_idx]
    eval_pairs = training_pairs[split_idx:] if split_idx < len(training_pairs) else train_pairs[-3:]
    
    # Format'la
    train_texts = [format_training_example(pair) for pair in train_pairs]
    eval_texts = [format_training_example(pair) for pair in eval_pairs]
    
    print(f"ğŸ“š EÄŸitim seti: {len(train_texts)} Ã¶rnek")
    print(f"ğŸ“ DeÄŸerlendirme seti: {len(eval_texts)} Ã¶rnek")
    
    # HuggingFace Dataset oluÅŸtur
    train_dataset = hf_datasets.Dataset.from_dict({"text": train_texts})
    eval_dataset = hf_datasets.Dataset.from_dict({"text": eval_texts})
    
    # Tokenize et - remove_columns doÄŸru ÅŸekilde
    train_dataset = train_dataset.map(
        lambda examples: tokenize_function(examples, tokenizer, max_length),
        batched=True,
        remove_columns=train_dataset.column_names,  # TÃ¼m orijinal sÃ¼tunlarÄ± kaldÄ±r
        desc="Tokenizing train dataset"
    )
    
    eval_dataset = eval_dataset.map(
        lambda examples: tokenize_function(examples, tokenizer, max_length),
        batched=True,
        remove_columns=eval_dataset.column_names,  # TÃ¼m orijinal sÃ¼tunlarÄ± kaldÄ±r
        desc="Tokenizing eval dataset"
    )
    
    return train_dataset, eval_dataset

class CustomTrainer(Trainer):
    """Ã–zelleÅŸtirilmiÅŸ Trainer"""
    
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """Loss hesaplama"""
        labels = inputs.get("labels")
        outputs = model(**inputs)
        
        # Shift so that tokens < n predict n
        shift_logits = outputs.logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        
        # Flatten the tokens
        loss_fct = torch.nn.CrossEntropyLoss()
        shift_logits = shift_logits.view(-1, shift_logits.size(-1))
        shift_labels = shift_labels.view(-1)
        
        # Enable model parallelism
        shift_labels = shift_labels.to(shift_logits.device)
        loss = loss_fct(shift_logits, shift_labels)
        
        return (loss, outputs) if return_outputs else loss

def debug_dataset(train_dataset, eval_dataset, tokenizer):
    """Dataset debug bilgileri"""
    print("\nğŸ” Dataset Debug Bilgileri:")
    print("-" * 40)
    
    # Train dataset
    print(f"ğŸ“š Train dataset boyutu: {len(train_dataset)}")
    print(f"ğŸ“š Train sÃ¼tunlarÄ±: {train_dataset.column_names}")
    
    # Ä°lk Ã¶rneÄŸi incele
    if len(train_dataset) > 0:
        first_example = train_dataset[0]
        print(f"ğŸ“ Ä°lk Ã¶rnek anahtarlarÄ±: {list(first_example.keys())}")
        
        if 'input_ids' in first_example:
            input_length = len(first_example['input_ids'])
            print(f"ğŸ“ Ä°lk Ã¶rnek input_ids uzunluÄŸu: {input_length}")
            
            # Ä°lk birkaÃ§ token'Ä± decode et
            if input_length > 0:
                sample_text = tokenizer.decode(first_example['input_ids'][:min(50, input_length)])
                print(f"ğŸ“„ Ä°lk 50 token Ã¶rneÄŸi: {sample_text[:100]}...")
    
    # Eval dataset
    print(f"ğŸ“ Eval dataset boyutu: {len(eval_dataset)}")
    print(f"ğŸ“ Eval sÃ¼tunlarÄ±: {eval_dataset.column_names}")
    
    # Uzunluk daÄŸÄ±lÄ±mÄ±
    if len(train_dataset) > 0 and 'input_ids' in train_dataset[0]:
        lengths = [len(example['input_ids']) for example in train_dataset]
        print(f"ğŸ“Š Uzunluk istatistikleri:")
        print(f"   Min: {min(lengths)}, Max: {max(lengths)}, Ortalama: {sum(lengths)/len(lengths):.1f}")

def train_understanding_model(
    base_model: str = "Qwen/Qwen2.5-3B-Instruct",
    output_dir: str = "./qwen_understanding_model",
    include_real_data: bool = False,
    max_seq_length: int = 512
):
    """Ana eÄŸitim fonksiyonu - DÃœZELTILMIÅ"""
    
    print(f"ğŸš€ Fine-tuning baÅŸlÄ±yor...")
    print(f"ğŸ“‹ Base model: {base_model}")
    print(f"ğŸ“ Output: {output_dir}")
    
    # 1. Model ve tokenizer yÃ¼kle
    print("ğŸ“¥ Model yÃ¼kleniyor...")
    hf_token = os.environ.get("HF_TOKEN")
    
    tokenizer = AutoTokenizer.from_pretrained(
        base_model, 
        use_fast=True, 
        token=hf_token,
        trust_remote_code=True
    )
    
    # Pad token ekle - QWEN iÃ§in Ã¶nemli
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})
    
    # Model yÃ¼kleme
    device_map = "auto" if torch.cuda.is_available() else None
    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch_dtype,
        device_map=device_map,
        token=hf_token,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    # Tokenizer resize (pad token ekledik)
    if len(tokenizer) > model.config.vocab_size:
        model.resize_token_embeddings(len(tokenizer))
    
    # 2. LoRA uygula
    print("ğŸ”§ LoRA konfigÃ¼rasyonu uygulanÄ±yor...")
    lora_config = create_optimized_lora_config()
    
    # Model'i PEFT iÃ§in hazÄ±rla
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()
    
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # 3. EÄŸitim verisi Ã¼ret
    print("ğŸ“Š EÄŸitim verisi Ã¼retiliyor...")
    es_client = None
    if include_real_data:
        try:
            es_client = ElasticsearchClient()
        except Exception as e:
            print(f"âš ï¸ Elasticsearch baÄŸlantÄ±sÄ± kurulamadÄ±, sadece sentetik veri kullanÄ±lacak: {e}")
    
    generator = ImprovedUnderstandingDataGenerator(es_client)
    training_pairs = generator.generate_comprehensive_training_data()
    
    if len(training_pairs) < 5:
        print("âŒ EÄŸitim verisi yetersiz!")
        return None
    
    # 4. Dataset hazÄ±rla
    print("ğŸ—‚ï¸ Dataset hazÄ±rlanÄ±yor...")
    train_dataset, eval_dataset = prepare_dataset(training_pairs, tokenizer, max_seq_length)
    
    # 5. Debug bilgileri
    debug_dataset(train_dataset, eval_dataset, tokenizer)
    
    # 6. DÃœZELTILMIÅ Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM iÃ§in
        pad_to_multiple_of=8 if torch.cuda.is_available() else None,
        return_tensors="pt"
    )
    
    # 7. Training arguments
    training_args = create_fixed_training_args(output_dir)
    
    # 8. Trainer oluÅŸtur
    print("ğŸ¯ Trainer oluÅŸturuluyor...")
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # 9. EÄŸitimi Ã§alÄ±ÅŸtÄ±r
    print("ğŸƒâ€â™‚ï¸ EÄŸitim baÅŸlatÄ±lÄ±yor...")
    try:
        trainer.train()
        print("âœ… EÄŸitim tamamlandÄ±!")
        
        # Model ve tokenizer'Ä± kaydet
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        print(f"ğŸ’¾ Model kaydedildi: {output_dir}")
        
        # Merge edilmiÅŸ versiyonu da kaydet
        try:
            print("ğŸ”— Model merge ediliyor...")
            merged_dir = os.path.join(output_dir, "merged")
            os.makedirs(merged_dir, exist_ok=True)
            
            # Merge ve kaydet
            merged_model = model.merge_and_unload()
            merged_model.save_pretrained(merged_dir)
            tokenizer.save_pretrained(merged_dir)
            
            print(f"âœ… Merge edilmiÅŸ model kaydedildi: {merged_dir}")
            
        except Exception as merge_error:
            print(f"âš ï¸ Model merge edilemedi: {merge_error}")
            print("ğŸ’¡ LoRA adapter modeli kullanÄ±labilir durumda.")
        
        return output_dir
        
    except Exception as training_error:
        print(f"âŒ EÄŸitim hatasÄ±: {training_error}")
        import traceback
        traceback.print_exc()
        raise

def test_trained_model(model_path: str):
    """EÄŸitilmiÅŸ modeli test et"""
    print(f"\nğŸ§ª Model testi baÅŸlÄ±yor: {model_path}")
    
    try:
        # Test iÃ§in merged model varsa onu kullan
        merged_path = os.path.join(model_path, "merged")
        test_path = merged_path if os.path.isdir(merged_path) else model_path
        
        print(f"ğŸ“ Test edilen model: {test_path}")
        
        tokenizer = AutoTokenizer.from_pretrained(test_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            test_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # Test sorularÄ±
        test_questions = [
            "CPU %85 yÃ¼ksek mi?",
            "VPN aktif ne demek?", 
            "son 2 saatte sistem nasÄ±ldÄ±?",
            "RAM %95 ne anlama gelir?",
            "internet hÄ±zÄ±m nasÄ±l?"
        ]
        
        print("\nğŸ“‹ Test sonuÃ§larÄ±:")
        print("-" * 60)
        
        for i, question in enumerate(test_questions, 1):
            # Qwen format
            prompt = f"<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
            
            inputs = tokenizer(prompt, return_tensors="pt")
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=80,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.9,
                    eos_token_id=tokenizer.eos_token_id,
                    pad_token_id=tokenizer.pad_token_id,
                    repetition_penalty=1.1
                )
            
            response = tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            print(f"{i}. â“ {question}")
            print(f"   ğŸ¤– {response}")
            print()
        
        print("âœ… Model test tamamlandÄ±!")
        return True
        
    except Exception as e:
        print(f"âŒ Model test hatasÄ±: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_dependencies():
    """Gerekli kÃ¼tÃ¼phaneleri kontrol et"""
    required_packages = ['torch', 'transformers', 'peft', 'datasets']
    missing = []
    
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing.append(package)
    
    if missing:
        print(f"âŒ Eksik kÃ¼tÃ¼phaneler bulundu: {', '.join(missing)}")
        print("ğŸ’¡ Kurulum iÃ§in:")
        print(f"   pip install {' '.join(missing)}")
        return False
    
    print("âœ… TÃ¼m kÃ¼tÃ¼phaneler yÃ¼klÃ¼.")
    return True


def main():
    """Anlama (understanding) odaklÄ± fine-tuning'i Ã§alÄ±ÅŸtÄ±r ve testi yap"""
    if not check_dependencies():
        return
    
    # EÄŸitim
    output_dir = train_understanding_model(
        base_model="Qwen/Qwen2.5-3B-Instruct",
        output_dir="./qwen_understanding_model",
        include_real_data=True,
        max_seq_length=512,
    )
    
    # Test
    if output_dir:
        test_trained_model(output_dir)


if __name__ == "__main__":
    main()